{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cd0a96",
   "metadata": {},
   "source": [
    "# HP1 Plus and No HP1 models\n",
    "\n",
    "Goal: Train elastic net regression models on 'HP1 Plus' and 'No HP1' datasets. Compare performance/interpretations to permutation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cd1d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c1b557",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Read in the 'HP1 plus' and 'No HP1' model datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7defb839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FBgn</th>\n",
       "      <th>log_TPM</th>\n",
       "      <th>HP1a</th>\n",
       "      <th>HP1B</th>\n",
       "      <th>HP1C</th>\n",
       "      <th>Mean_Accesibility</th>\n",
       "      <th>state.name_1</th>\n",
       "      <th>state.name_2</th>\n",
       "      <th>state.name_3</th>\n",
       "      <th>state.name_4</th>\n",
       "      <th>...</th>\n",
       "      <th>HP1C.GAGA_Sites</th>\n",
       "      <th>HP1C.GAGA_Avg_Score</th>\n",
       "      <th>HP1C.DRE_Sites</th>\n",
       "      <th>HP1C.DRE_Avg_Score</th>\n",
       "      <th>HP1C.Disco_Sites</th>\n",
       "      <th>HP1C.Disco_Avg_Score</th>\n",
       "      <th>A_B</th>\n",
       "      <th>A_C</th>\n",
       "      <th>B_C</th>\n",
       "      <th>A_B_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FBgn0062565</td>\n",
       "      <td>0.197161</td>\n",
       "      <td>0.596583</td>\n",
       "      <td>-0.769971</td>\n",
       "      <td>-0.769417</td>\n",
       "      <td>2.04800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.538834</td>\n",
       "      <td>-4.313398</td>\n",
       "      <td>-0.769417</td>\n",
       "      <td>-6.516731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.459351</td>\n",
       "      <td>-0.459021</td>\n",
       "      <td>0.592429</td>\n",
       "      <td>0.353433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FBgn0053217</td>\n",
       "      <td>1.255029</td>\n",
       "      <td>2.769302</td>\n",
       "      <td>0.931543</td>\n",
       "      <td>1.072085</td>\n",
       "      <td>3.53125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.579725</td>\n",
       "      <td>2.968927</td>\n",
       "      <td>0.998694</td>\n",
       "      <td>2.765684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FBgn0040372</td>\n",
       "      <td>0.622380</td>\n",
       "      <td>0.442630</td>\n",
       "      <td>1.026500</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>2.59850</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.162549</td>\n",
       "      <td>1.466298</td>\n",
       "      <td>0.775032</td>\n",
       "      <td>3.534617</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>5.225230</td>\n",
       "      <td>0.454360</td>\n",
       "      <td>0.171526</td>\n",
       "      <td>0.397786</td>\n",
       "      <td>0.176072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBgn0000316</td>\n",
       "      <td>1.879004</td>\n",
       "      <td>0.576955</td>\n",
       "      <td>0.305769</td>\n",
       "      <td>0.251194</td>\n",
       "      <td>0.76900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753582</td>\n",
       "      <td>1.261552</td>\n",
       "      <td>0.502388</td>\n",
       "      <td>3.825009</td>\n",
       "      <td>0.251194</td>\n",
       "      <td>3.095365</td>\n",
       "      <td>0.176415</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.076807</td>\n",
       "      <td>0.044314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FBgn0024989</td>\n",
       "      <td>-4.075113</td>\n",
       "      <td>0.611478</td>\n",
       "      <td>1.393595</td>\n",
       "      <td>1.637319</td>\n",
       "      <td>2.12500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.274638</td>\n",
       "      <td>8.752215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.274638</td>\n",
       "      <td>17.020193</td>\n",
       "      <td>0.852153</td>\n",
       "      <td>1.001185</td>\n",
       "      <td>2.281760</td>\n",
       "      <td>1.395246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 429 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          FBgn   log_TPM      HP1a      HP1B      HP1C  Mean_Accesibility  \\\n",
       "0  FBgn0062565  0.197161  0.596583 -0.769971 -0.769417            2.04800   \n",
       "1  FBgn0053217  1.255029  2.769302  0.931543  1.072085            3.53125   \n",
       "2  FBgn0040372  0.622380  0.442630  1.026500  0.387516            2.59850   \n",
       "3  FBgn0000316  1.879004  0.576955  0.305769  0.251194            0.76900   \n",
       "4  FBgn0024989 -4.075113  0.611478  1.393595  1.637319            2.12500   \n",
       "\n",
       "   state.name_1  state.name_2  state.name_3  state.name_4  ...  \\\n",
       "0             0             0             0             0  ...   \n",
       "1             0             0             0             0  ...   \n",
       "2             0             0             0             0  ...   \n",
       "3             0             0             0             0  ...   \n",
       "4             0             0             0             1  ...   \n",
       "\n",
       "   HP1C.GAGA_Sites  HP1C.GAGA_Avg_Score  HP1C.DRE_Sites  HP1C.DRE_Avg_Score  \\\n",
       "0        -1.538834            -4.313398       -0.769417           -6.516731   \n",
       "1         0.000000             0.000000        0.000000            0.000000   \n",
       "2         1.162549             1.466298        0.775032            3.534617   \n",
       "3         0.753582             1.261552        0.502388            3.825009   \n",
       "4         3.274638             8.752215        0.000000            0.000000   \n",
       "\n",
       "   HP1C.Disco_Sites  HP1C.Disco_Avg_Score       A_B       A_C       B_C  \\\n",
       "0          0.000000              0.000000 -0.459351 -0.459021  0.592429   \n",
       "1          0.000000              0.000000  2.579725  2.968927  0.998694   \n",
       "2          0.387516              5.225230  0.454360  0.171526  0.397786   \n",
       "3          0.251194              3.095365  0.176415  0.144928  0.076807   \n",
       "4          3.274638             17.020193  0.852153  1.001185  2.281760   \n",
       "\n",
       "      A_B_C  \n",
       "0  0.353433  \n",
       "1  2.765684  \n",
       "2  0.176072  \n",
       "3  0.044314  \n",
       "4  1.395246  \n",
       "\n",
       "[5 rows x 429 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset includes model data\n",
    "plus_model = pd.read_csv('../datasets/model_inputs/hp1_plus_input.csv')\n",
    "no_model = pd.read_csv('../datasets/model_inputs/no_hp1_input.csv')\n",
    "\n",
    "plus_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2562a0b",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "As before, split the datasets before doing any other work. Keep a copy of data that includes FBgn identifiers which can be used for downstream analyses, but need to be excluded from model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2ecd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plus_copy = plus_model.copy()\n",
    "no_copy = no_model.copy()\n",
    "# Remove these variables until they are scaled properly\n",
    "#genes_copy = genes_copy.drop(['Length', 'PInd'], axis = 1)\n",
    "\n",
    "for set_ in (plus_copy, no_copy):\n",
    "    set_.drop('log_TPM', axis=1, inplace=True)\n",
    "\n",
    "plus_train, plus_test, yplus_train, yplus_test = train_test_split(plus_copy, plus_model['log_TPM'],\n",
    "                                                                test_size = 0.2,\n",
    "                                                                 random_state = 42)\n",
    "plus_train.head()\n",
    "\n",
    "no_train, no_test, yno_train, yno_test = train_test_split(no_copy, no_model['log_TPM'],\n",
    "                                                         test_size=0.2, random_state=42)\n",
    "\n",
    "plus_train2 = plus_train.drop(['FBgn'], axis = 1)\n",
    "plus_test2 = plus_test.drop(['FBgn'], axis = 1)\n",
    "no_train2 = no_train.drop(['FBgn'], axis = 1)\n",
    "no_test2 = no_test.drop(['FBgn'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46991e39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test to retrieve chromatin state column indices\n",
    "# Need this for column transformer\n",
    "cstate_indices = []\n",
    "for i in range(9):\n",
    "    j = str(i+1)\n",
    "    var_name = 'state.name_'+j\n",
    "    k = plus_train2.columns.get_loc(var_name)\n",
    "    cstate_indices.append(k)\n",
    "\n",
    "other_indices = []\n",
    "for i in range(len(plus_train2.columns)):\n",
    "    if(i in cstate_indices):\n",
    "        continue\n",
    "    else:\n",
    "        other_indices.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c464b1",
   "metadata": {},
   "source": [
    "## Data Transformations and Grid Search\n",
    "\n",
    "All the numerical variables need to be scaled and zero-centered. Chromatin state is already one-hot encoded and doesn't need this. After selectively scaling the numerical variables, the next step is to perform grid search to fine-tune hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2366324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['transformer', 'model']\n",
      "parameters:\n",
      "{'model__alpha': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
      " 'model__l1_ratio': [0.1, 0.25, 0.5, 0.75, 0.9]}\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13.576165314655555, tolerance: 1.7380245312699107\n",
      "  positive)\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:   12.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 13.223s\n",
      "\n",
      "Best score: 0.566\n",
      "Best parameters set:\n",
      "\tmodel__alpha: 0.1\n",
      "\tmodel__l1_ratio: 0.1\n",
      "Performing grid search...\n",
      "pipeline: ['transformer', 'model']\n",
      "parameters:\n",
      "{'model__alpha': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
      " 'model__l1_ratio': [0.1, 0.25, 0.5, 0.75, 0.9]}\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.432s\n",
      "\n",
      "Best score: 0.518\n",
      "Best parameters set:\n",
      "\tmodel__alpha: 0.1\n",
      "\tmodel__l1_ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "parameters = {'model__alpha': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "             'model__l1_ratio': [0.1, 0.25, 0.5, 0.75, 0.9]}\n",
    "\n",
    "steps = list()\n",
    "steps.append(['transformer', ColumnTransformer(transformers = [('num', StandardScaler(),\n",
    "                                                              other_indices)],\n",
    "                                              remainder='passthrough')])\n",
    "steps.append(['model', ElasticNet()])\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plus_grid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "\n",
    "    t0 = time()\n",
    "    plus_grid_search.fit(plus_train2, yplus_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % plus_grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = plus_grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Create a new pipeline for the no hp1 model\n",
    "# Need to identify indices for column transformer\n",
    "no_cstate_indices = []\n",
    "for i in range(9):\n",
    "    j = str(i+1)\n",
    "    var_name = 'state.name_'+j\n",
    "    k = no_train2.columns.get_loc(var_name)\n",
    "    no_cstate_indices.append(k)\n",
    "\n",
    "no_other_indices = []\n",
    "for i in range(len(no_train2.columns)):\n",
    "    if(i in cstate_indices):\n",
    "        continue\n",
    "    else:\n",
    "        no_other_indices.append(i)\n",
    "\n",
    "nosteps = list()\n",
    "nosteps.append(['transformer', ColumnTransformer(transformers = [('num', StandardScaler(),\n",
    "                                                                 no_other_indices)],\n",
    "                                                remainder='passthrough')])\n",
    "nosteps.append(['model', ElasticNet()])\n",
    "no_pipeline = Pipeline(steps = nosteps)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    no_grid_search = GridSearchCV(no_pipeline, parameters, n_jobs=1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "\n",
    "    t0 = time()\n",
    "    no_grid_search.fit(no_train2, yno_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % no_grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_no_parameters = no_grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_no_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc92dce",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "\n",
    "Now that the model parameters are tuned, generate predictions for both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ad4299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.798644706419509, tolerance: 2.3869830377915533\n",
      "  positive)\n",
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.78899508637187, tolerance: 2.4142506084724134\n",
      "  positive)\n",
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.712968546000411, tolerance: 2.443193948958114\n",
      "  positive)\n",
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.585399073537701, tolerance: 2.4494991147677863\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "plus_ytrain_pred = cross_val_predict(plus_grid_search.best_estimator_, plus_train2, \n",
    "                                     yplus_train, cv=10)\n",
    "\n",
    "plus_ytest_pred = plus_grid_search.best_estimator_.predict(plus_test2)\n",
    "\n",
    "no_ytrain_pred = cross_val_predict(no_grid_search.best_estimator_, no_train2,\n",
    "                                 yno_train, cv=10)\n",
    "no_ytest_pred = no_grid_search.best_estimator_.predict(no_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56555cf",
   "metadata": {},
   "source": [
    "## Extract Coefficients\n",
    "\n",
    "From the best fit model, extract coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e13246e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Mean_Accesibility', 'state.name_1', 'state.name_2', 'state.name_3',\n",
      "       'state.name_4', 'state.name_5', 'state.name_6', 'state.name_7',\n",
      "       'state.name_8', 'state.name_9',\n",
      "       ...\n",
      "       'TTG', 'TTT', 'exon_num', 'density', 'GAGA_Sites', 'GAGA_Avg_Score',\n",
      "       'DRE_Sites', 'DRE_Avg_Score', 'Disco_Sites', 'Disco_Avg_Score'],\n",
      "      dtype='object', length=105)\n"
     ]
    }
   ],
   "source": [
    "plus_coefs = pd.DataFrame({'feature_names' : plus_train2.columns,\n",
    "'feature_coefs' : plus_grid_search.best_estimator_.named_steps['model'].coef_})\n",
    "plus_coefs.head()\n",
    "\n",
    "no_coefs = pd.DataFrame({'feature_names': no_train2.columns,\n",
    "                        'feature_coefs': no_grid_search.best_estimator_.named_steps['model'].coef_})\n",
    "print(no_train2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9154b6c",
   "metadata": {},
   "source": [
    "## Write Output\n",
    "\n",
    "Now write out the prediction and coefficient datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91769f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plus_coefs.to_csv('../datasets/model_outputs/HP1_plus_coefficients_no_RFE.csv',\n",
    "                 index = False)\n",
    "plus_train_preds = pd.DataFrame({'FBgn': plus_train['FBgn'],\n",
    "                                'Y_true': yplus_train,\n",
    "                                'Y_pred': plus_ytrain_pred})\n",
    "plus_train_preds.to_csv('../datasets/model_outputs/HP1_plus_train_predictions_no_RFE.csv',\n",
    "                       index = False)\n",
    "\n",
    "no_coefs.to_csv('../datasets/model_outputs/No_HP1_coefficients_no_RFE.csv',\n",
    "               index = False)\n",
    "no_train_preds = pd.DataFrame({'FBgn': no_train['FBgn'],\n",
    "                              'Y_true': yno_train,\n",
    "                              'Y_pred': no_ytrain_pred})\n",
    "no_train_preds.to_csv('../datasets/model_outputs/No_HP1_train_predictions_no_RFE.csv',\n",
    "                     index=False)\n",
    "\n",
    "plus_test_preds = pd.DataFrame({'FBgn': plus_test['FBgn'],\n",
    "                               'Y_true': yplus_test,\n",
    "                               'Y_pred': plus_ytest_pred})\n",
    "plus_test_preds.to_csv('../datasets/model_outputs/HP1_plus_test_predictions_no_RFE.csv',\n",
    "                      index=False)\n",
    "\n",
    "no_test_preds = pd.DataFrame({'FBgn': no_test['FBgn'],\n",
    "                             'Y_true': yno_test,\n",
    "                             'Y_pred': no_ytest_pred})\n",
    "no_test_preds.to_csv('../datasets/model_outputs/No_HP1_test_predictions_no_RFE.csv',\n",
    "                    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d47d9",
   "metadata": {},
   "source": [
    "## Permutations\n",
    "\n",
    "Now, last step is to run through model fitting, generate predictions and extract coefficients with the permuted datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdf5db5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['transformer', 'model']\n",
      "parameters:\n",
      "{'model__alpha': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
      " 'model__l1_ratio': [0.1, 0.25, 0.5, 0.75, 0.9]}\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    4.7s finished\n",
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.791s\n",
      "\n",
      "Best score: 0.081\n",
      "Best parameters set:\n",
      "\tmodel__alpha: 0.9\n",
      "\tmodel__l1_ratio: 0.1\n",
      "Performing grid search...\n",
      "pipeline: ['transformer', 'model']\n",
      "parameters:\n",
      "{'model__alpha': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
      " 'model__l1_ratio': [0.1, 0.25, 0.5, 0.75, 0.9]}\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.731s\n",
      "\n",
      "Best score: 0.000\n",
      "Best parameters set:\n",
      "\tmodel__alpha: 0.1\n",
      "\tmodel__l1_ratio: 0.9\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "plus_perm = pd.read_csv('../datasets/model_inputs/HP1_Plus_Permuted_Input.csv')\n",
    "no_perm = pd.read_csv('../datasets/model_inputs/No_HP1_Permuted_Inputs.csv')\n",
    "\n",
    "# train test split\n",
    "plus_copy = plus_model.copy()\n",
    "no_copy = no_model.copy()\n",
    "\n",
    "pperm_copy = plus_perm.copy()\n",
    "nperm_copy = no_perm.copy()\n",
    "\n",
    "for set_ in (pperm_copy, nperm_copy):\n",
    "    set_.drop('log_TPM', axis=1, inplace=True)\n",
    "\n",
    "pperm_train, pperm_test, ypp_train, ypp_test = train_test_split(pperm_copy, plus_perm['log_TPM'],\n",
    "                                                               test_size = 0.2, random_state=42)\n",
    "nperm_train, nperm_test, ynp_train, ynp_test = train_test_split(nperm_copy, no_perm['log_TPM'],\n",
    "                                                               test_size = 0.2, random_state=42)\n",
    "# pipeline, parameter tuning, model fit\n",
    "steps = list()\n",
    "steps.append(('scaler', StandardScaler()))\n",
    "steps.append(('model', ElasticNet()))\n",
    "perm_pipeline = Pipeline(steps = steps)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pperm_grid_search = GridSearchCV(perm_pipeline, parameters, n_jobs=1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "\n",
    "    t0 = time()\n",
    "    pperm_grid_search.fit(pperm_train, ypp_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % pperm_grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = pperm_grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nperm_grid_search = GridSearchCV(perm_pipeline, parameters, n_jobs=1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "\n",
    "    t0 = time()\n",
    "    nperm_grid_search.fit(nperm_train, ynp_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % nperm_grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = nperm_grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# generate predictions\n",
    "ypp_train_pred = cross_val_predict(pperm_grid_search.best_estimator_, pperm_train,\n",
    "                                  ypp_train, cv=10)\n",
    "ypp_test_pred = pperm_grid_search.best_estimator_.predict(pperm_test)\n",
    "\n",
    "ynp_train_pred = cross_val_predict(nperm_grid_search.best_estimator_, nperm_train,\n",
    "                                  ynp_train, cv=10)\n",
    "ynp_test_pred = nperm_grid_search.best_estimator_.predict(nperm_test)\n",
    "\n",
    "# extract coefficients\n",
    "pperm_coefs = pd.DataFrame({'Feature_Name': pperm_train.columns,\n",
    "                           'Feature_Coef': pperm_grid_search.best_estimator_.named_steps['model'].coef_})\n",
    "\n",
    "nperm_coefs = pd.DataFrame({'Feature_Name': nperm_train.columns,\n",
    "                           'Feature_Coef': nperm_grid_search.best_estimator_.named_steps['model'].coef_})\n",
    "\n",
    "# write output\n",
    "pperm_preds_train = pd.DataFrame({'Y_true': ypp_train, 'Y_pred': ypp_train_pred})\n",
    "pperm_preds_test = pd.DataFrame({'Y_true': ypp_test, 'Y_pred': ypp_test_pred})\n",
    "nperm_preds_train = pd.DataFrame({'Y_true': ynp_train, 'Y_pred': ynp_train_pred})\n",
    "nperm_preds_test = pd.DataFrame({'Y_true': ynp_test, 'Y_pred': ynp_test_pred})\n",
    "\n",
    "pperm_coefs.to_csv('../datasets/model_outputs/HP1_Plus_Permutations_Coefs.csv', index=False)\n",
    "nperm_coefs.to_csv('../datasets/model_outputs/No_HP1_Permutations_Coefs.csv', index=False)\n",
    "\n",
    "pperm_preds_train.to_csv('../datasets/model_outputs/HP1_Plus_Permutation_TrainPreds.csv',\n",
    "                        index=False)\n",
    "pperm_preds_test.to_csv('../datasets/model_outputs/HP1_Plus_Permutation_TestPreds.csv',\n",
    "                       index=False)\n",
    "\n",
    "nperm_preds_train.to_csv('../datasets/model_outputs/No_HP1_Permutation_TrainPreds.csv',\n",
    "                        index=False)\n",
    "nperm_preds_test.to_csv('../datasets/model_outputs/No_HP1_Permutation_TestPreds.csv',\n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf66bb",
   "metadata": {},
   "source": [
    "## Main effects only\n",
    "\n",
    "Train a model without interaction effects and examine coefficient values and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1570498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['transformer', 'model']\n",
      "parameters:\n",
      "{'model__alpha': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
      " 'model__l1_ratio': [0.1, 0.25, 0.5, 0.75, 0.9]}\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.318s\n",
      "\n",
      "Best score: 0.556\n",
      "Best parameters set:\n",
      "\tmodel__alpha: 0.1\n",
      "\tmodel__l1_ratio: 0.1\n"
     ]
    }
   ],
   "source": [
    "mains = pd.read_csv('../datasets/model_inputs/main_effects_only_plus_inputs.csv')\n",
    "mains_copy = mains.copy()\n",
    "mains_copy.head()\n",
    "# Remove these variables until they are scaled properly\n",
    "#genes_copy = genes_copy.drop(['Length', 'PInd'], axis = 1)\n",
    "mains_copy.drop('log_TPM', axis=1, inplace=True)\n",
    "\n",
    "mains_train, mains_test, ymain_train, ymain_test = train_test_split(mains_copy, mains['log_TPM'],\n",
    "                                                                test_size = 0.2,\n",
    "                                                                 random_state = 42)\n",
    "\n",
    "\n",
    "mains_train2 = mains_train.drop(['FBgn'], axis = 1)\n",
    "\n",
    "cstate_indices = []\n",
    "for i in range(9):\n",
    "    j = str(i+1)\n",
    "    var_name = 'state.name_'+j\n",
    "    k = mains_train2.columns.get_loc(var_name)\n",
    "    cstate_indices.append(k)\n",
    "\n",
    "other_indices = []\n",
    "for i in range(len(mains_train2.columns)):\n",
    "    if(i in cstate_indices):\n",
    "        continue\n",
    "    else:\n",
    "        other_indices.append(i)\n",
    "        \n",
    "parameters = {'model__alpha': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "             'model__l1_ratio': [0.1, 0.25, 0.5, 0.75, 0.9]}\n",
    "\n",
    "steps = list()\n",
    "steps.append(['transformer', ColumnTransformer(transformers = [('num', StandardScaler(),\n",
    "                                                              other_indices)],\n",
    "                                              remainder='passthrough')])\n",
    "steps.append(['model', ElasticNet()])\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mains_grid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "\n",
    "    t0 = time()\n",
    "    mains_grid_search.fit(mains_train2, ymain_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % mains_grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = mains_grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "mains_ytrain_pred = cross_val_predict(mains_grid_search.best_estimator_, mains_train2, \n",
    "                                     ymain_train, cv=10)\n",
    "\n",
    "mains_coefs = pd.DataFrame({'feature_names' : mains_train2.columns,\n",
    "'feature_coefs' : mains_grid_search.best_estimator_.named_steps['model'].coef_})\n",
    "mains_coefs.head()\n",
    "\n",
    "\n",
    "mains_coefs.to_csv('../datasets/model_outputs/HP1_plus_coefficients_main_effects_only.csv',\n",
    "                 index = False)\n",
    "mains_train_preds = pd.DataFrame({'FBgn': mains_train['FBgn'],\n",
    "                                'Y_true': ymain_train,\n",
    "                                'Y_pred': mains_ytrain_pred})\n",
    "plus_train_preds.to_csv('../datasets/model_outputs/HP1_plus_train_predictions_main_effects_only.csv',\n",
    "                       index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17e09059",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulated data\n",
    "a_cpr11b = pd.read_csv('../datasets/dcas9_simulations/cpr11b_hp1a_hp1_plus_inputs.csv')\n",
    "b_cpr11b = pd.read_csv('../datasets/dcas9_simulations/cpr11b_hp1b_hp1_plus_inputs.csv')\n",
    "c_cpr11b = pd.read_csv('../datasets/dcas9_simulations/cpr11b_hp1c_hp1_plus_inputs.csv')\n",
    "\n",
    "a_dgt3 = pd.read_csv('../datasets/dcas9_simulations/dgt3_hp1a_hp1_plus_inputs.csv')\n",
    "b_dgt3 = pd.read_csv('../datasets/dcas9_simulations/dgt3_hp1b_hp1_plus_inputs.csv')\n",
    "c_dgt3 = pd.read_csv('../datasets/dcas9_simulations/dgt3_hp1c_hp1_plus_inputs.csv')\n",
    "\n",
    "a_ceca1 = pd.read_csv('../datasets/dcas9_simulations/ceca1_hp1a_hp1_plus_inputs.csv')\n",
    "b_ceca1 = pd.read_csv('../datasets/dcas9_simulations/ceca1_hp1b_hp1_plus_inputs.csv')\n",
    "c_ceca1 = pd.read_csv('../datasets/dcas9_simulations/ceca1_hp1c_hp1_plus_inputs.csv')\n",
    "\n",
    "a_mtk = pd.read_csv('../datasets/dcas9_simulations/mtk_hp1a_hp1_plus_inputs.csv')\n",
    "b_mtk = pd.read_csv('../datasets/dcas9_simulations/mtk_hp1b_hp1_plus_inputs.csv')\n",
    "c_mtk = pd.read_csv('../datasets/dcas9_simulations/mtk_hp1c_hp1_plus_inputs.csv')\n",
    "\n",
    "a_egr = pd.read_csv('../datasets/dcas9_simulations/egr_hp1a_hp1_plus_inputs.csv')\n",
    "a_pyr = pd.read_csv('../datasets/dcas9_simulations/pyr_hp1a_hp1_plus_inputs.csv')\n",
    "a_mats = pd.read_csv('../datasets/dcas9_simulations/mats_hp1a_hp1_plus_inputs.csv')\n",
    "\n",
    "b_rab3 = pd.read_csv('../datasets/dcas9_simulations/rab3_hp1b_hp1_plus_inputs.csv')\n",
    "b_shaw = pd.read_csv('../datasets/dcas9_simulations/shaw_hp1b_hp1_plus_inputs.csv')\n",
    "b_cg76 = pd.read_csv('../datasets/dcas9_simulations/cg76_hp1b_hp1_plus_inputs.csv')\n",
    "\n",
    "c_alpha = pd.read_csv('../datasets/dcas9_simulations/alpha_hp1c_hp1_plus_inputs.csv')\n",
    "c_cg26 = pd.read_csv('../datasets/dcas9_simulations/cg26_hp1c_hp1_plus_inputs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "501ee9d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_cpr11b_preds = plus_grid_search.best_estimator_.predict(a_cpr11b)\n",
    "b_cpr11b_preds = plus_grid_search.best_estimator_.predict(b_cpr11b)\n",
    "c_cpr11b_preds = plus_grid_search.best_estimator_.predict(c_cpr11b)\n",
    "\n",
    "a_dgt3_preds = plus_grid_search.best_estimator_.predict(a_dgt3)\n",
    "b_dgt3_preds = plus_grid_search.best_estimator_.predict(b_dgt3)\n",
    "c_dgt3_preds = plus_grid_search.best_estimator_.predict(c_dgt3)\n",
    "\n",
    "a_ceca1_preds = plus_grid_search.best_estimator_.predict(a_ceca1)\n",
    "b_ceca1_preds = plus_grid_search.best_estimator_.predict(b_ceca1)\n",
    "c_ceca1_preds = plus_grid_search.best_estimator_.predict(c_ceca1)\n",
    "\n",
    "a_mtk_preds = plus_grid_search.best_estimator_.predict(a_mtk)\n",
    "b_mtk_preds = plus_grid_search.best_estimator_.predict(b_mtk)\n",
    "c_mtk_preds = plus_grid_search.best_estimator_.predict(c_mtk)\n",
    "\n",
    "a_egr_preds = plus_grid_search.best_estimator_.predict(a_egr)\n",
    "a_pyr_preds = plus_grid_search.best_estimator_.predict(a_pyr)\n",
    "a_mats_preds = plus_grid_search.best_estimator_.predict(a_mats)\n",
    "\n",
    "b_rab3_preds = plus_grid_search.best_estimator_.predict(b_rab3)\n",
    "b_shaw_preds = plus_grid_search.best_estimator_.predict(b_shaw)\n",
    "b_cg76_preds = plus_grid_search.best_estimator_.predict(b_cg76)\n",
    "\n",
    "c_alpha_preds = plus_grid_search.best_estimator_.predict(c_alpha)\n",
    "c_cg26_preds = plus_grid_search.best_estimator_.predict(c_cg26)\n",
    "\n",
    "cpr11b_out = pd.DataFrame({'HP1a': a_cpr11b_preds,\n",
    "                          'HP1B': b_cpr11b_preds,\n",
    "                          'HP1C': c_cpr11b_preds})\n",
    "dgt3_out = pd.DataFrame({'HP1a': a_dgt3_preds,\n",
    "                        'HP1B': b_dgt3_preds,\n",
    "                        'HP1C': c_dgt3_preds})\n",
    "ceca1_out = pd.DataFrame({'HP1a': a_ceca1_preds,\n",
    "                         'HP1B': b_ceca1_preds,\n",
    "                         'HP1C': c_ceca1_preds})\n",
    "mtk_out = pd.DataFrame({'HP1a': a_mtk_preds,\n",
    "                       'HP1B': b_mtk_preds,\n",
    "                       'HP1C': c_mtk_preds})\n",
    "\n",
    "hp1a_out = pd.DataFrame({'egr': a_egr_preds,\n",
    "                       'pyr': a_pyr_preds,\n",
    "                       'mats': a_mats_preds})\n",
    "hp1b_out = pd.DataFrame({'rab3': b_rab3_preds,\n",
    "                       'shaw': b_shaw_preds,\n",
    "                       'cg76': b_cg76_preds})\n",
    "hp1c_out = pd.DataFrame({'alpha': c_alpha_preds,\n",
    "                       'cg26': c_cg26_preds})\n",
    "\n",
    "cpr11b_out.to_csv('../datasets/model_outputs/cpr11b_hp1_plus_simulated.csv')\n",
    "dgt3_out.to_csv('../datasets/model_outputs/dgt3_hp1_plus_simulated.csv')\n",
    "ceca1_out.to_csv('../datasets/model_outputs/ceca1_hp1_plus_simulated.csv')\n",
    "mtk_out.to_csv('../datasets/model_outputs/mtk_hp1_plus_simulated.csv')\n",
    "hp1a_out.to_csv('../datasets/model_outputs/hp1a_hp1_plus_simulated.csv')\n",
    "hp1b_out.to_csv('../datasets/model_outputs/hp1b_hp1_plus_simulated.csv')\n",
    "hp1c_out.to_csv('../datasets/model_outputs/hp1c_hp1_plus_simulated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31c7991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-460.35890424 -145.66383712 -488.75583917 -309.61267959]\n"
     ]
    }
   ],
   "source": [
    "baselines = pd.read_csv('../datasets/model_inputs/HP1_Plus_dcas9_baselines.csv')\n",
    "baseline_preds = plus_grid_search.best_estimator_.predict(baselines)\n",
    "print(baseline_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "090feacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03137980730178763"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(nperm_coefs['Feature_Coef'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "256b5aff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-069ba02e30cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m rfe_selector = RFE(estimator=ElasticNet(alpha = 0.1, l1_ratio = 0.1),\n\u001b[1;32m     10\u001b[0m                    n_features_to_select = 15, step = 1)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrfe_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtrain_set2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrfe_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set2' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard Deviation:\", scores.std())\n",
    "    \n",
    "rfe_selector = RFE(estimator=ElasticNet(alpha = 0.1, l1_ratio = 0.1),\n",
    "                   n_features_to_select = 15, step = 1)\n",
    "rfe_selector.fit(train_set2, y_train)\n",
    "\n",
    "train_set2.columns[rfe_selector.get_support()]\n",
    "    \n",
    "#elastic_net = ElasticNet(alpha = 0.1, l1_ratio = .5)\n",
    "\n",
    "#scores = cross_val_score(elastic_net, train_set2, y_train,\n",
    "                        #scoring = \"neg_mean_squared_error\", cv=10)\n",
    "#net_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "#display_scores(net_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a58f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_train = train_set2[train_set2.columns[rfe_selector.get_support()]]\n",
    "enet = ElasticNet(alpha = 0.1, l1_ratio = 0.5)\n",
    "y_pred_plus_rfe = cross_val_predict(enet, rfe_train, y_train, cv=10)\n",
    "y_pred_plus = cross_val_predict(enet, train_set2, y_train, cv=10)\n",
    "\n",
    "Resid = y_pred_plus - y_train\n",
    "resid_rfe = y_pred_plus_rfe - y_train\n",
    "\n",
    "# Compare - did the RFE improve performance?\n",
    "print(sum(Resid**2))\n",
    "print(sum(resid_rfe**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d78626a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FBgn</th>\n",
       "      <th>HP1a</th>\n",
       "      <th>HP1B</th>\n",
       "      <th>HP1C</th>\n",
       "      <th>B_C</th>\n",
       "      <th>state.name_1</th>\n",
       "      <th>state.name_2</th>\n",
       "      <th>state.name_4</th>\n",
       "      <th>state.name_9</th>\n",
       "      <th>CTC</th>\n",
       "      <th>GGA</th>\n",
       "      <th>GTC</th>\n",
       "      <th>DRE_Sites</th>\n",
       "      <th>DRE_Avg_Score</th>\n",
       "      <th>Disco_Sites</th>\n",
       "      <th>Disco_Avg_Score</th>\n",
       "      <th>Y_true</th>\n",
       "      <th>Y_Pred_Full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7828</th>\n",
       "      <td>FBgn0260960</td>\n",
       "      <td>1.976924</td>\n",
       "      <td>3.629032</td>\n",
       "      <td>4.837500</td>\n",
       "      <td>17.555440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.422502</td>\n",
       "      <td>0.611836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>FBgn0052000</td>\n",
       "      <td>2.453688</td>\n",
       "      <td>0.973861</td>\n",
       "      <td>1.645251</td>\n",
       "      <td>1.602246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.097486</td>\n",
       "      <td>0.519560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8981</th>\n",
       "      <td>FBgn0004228</td>\n",
       "      <td>-1.228404</td>\n",
       "      <td>-0.178896</td>\n",
       "      <td>-0.256333</td>\n",
       "      <td>0.045857</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.34091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.276521</td>\n",
       "      <td>0.083403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>FBgn0005278</td>\n",
       "      <td>3.115202</td>\n",
       "      <td>2.214164</td>\n",
       "      <td>1.933721</td>\n",
       "      <td>4.281574</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.798414</td>\n",
       "      <td>0.869878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8810</th>\n",
       "      <td>FBgn0031053</td>\n",
       "      <td>0.784784</td>\n",
       "      <td>2.027456</td>\n",
       "      <td>1.988963</td>\n",
       "      <td>4.032536</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11.26515</td>\n",
       "      <td>3</td>\n",
       "      <td>6.23118</td>\n",
       "      <td>0.670263</td>\n",
       "      <td>1.327450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             FBgn      HP1a      HP1B      HP1C        B_C  state.name_1  \\\n",
       "7828  FBgn0260960  1.976924  3.629032  4.837500  17.555440             0   \n",
       "2456  FBgn0052000  2.453688  0.973861  1.645251   1.602246             0   \n",
       "8981  FBgn0004228 -1.228404 -0.178896 -0.256333   0.045857             0   \n",
       "2455  FBgn0005278  3.115202  2.214164  1.933721   4.281574             0   \n",
       "8810  FBgn0031053  0.784784  2.027456  1.988963   4.032536             0   \n",
       "\n",
       "      state.name_2  state.name_4  state.name_9  CTC  GGA  GTC  DRE_Sites  \\\n",
       "7828             0             0             0    1    1    0          0   \n",
       "2456             0             0             0    0    2    0          0   \n",
       "8981             1             0             0    5    2    0          2   \n",
       "2455             1             0             0    1    0    2          0   \n",
       "8810             0             0             0    0    1    3          2   \n",
       "\n",
       "      DRE_Avg_Score  Disco_Sites  Disco_Avg_Score    Y_true  Y_Pred_Full  \n",
       "7828        0.00000            0          0.00000  1.422502     0.611836  \n",
       "2456        0.00000            0          0.00000  0.097486     0.519560  \n",
       "8981        3.34091            0          0.00000 -1.276521     0.083403  \n",
       "2455        0.00000            0          0.00000  0.798414     0.869878  \n",
       "8810       11.26515            3          6.23118  0.670263     1.327450  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'FBgn': train_set['FBgn'],\n",
    "       'HP1a': train_set['HP1a'],\n",
    "       'HP1B': train_set['HP1B'],\n",
    "        'HP1C': train_set['HP1C'],\n",
    "       'B_C': train_set['B_C'],\n",
    "        'state.name_1': train_set['state.name_1'],\n",
    "        'state.name_2': train_set['state.name_2'],\n",
    "        'state.name_4': train_set['state.name_4'],\n",
    "        'state.name_9': train_set['state.name_9'],\n",
    "        'CTC': train_set['CTC'],\n",
    "        'GGA': train_set['GGA'],\n",
    "        'GTC': train_set['GTC'],\n",
    "        'DRE_Sites': train_set['DRE_Sites'],\n",
    "        'DRE_Avg_Score': train_set['DRE_Avg_Score'],\n",
    "        'Disco_Sites': train_set['Disco_Sites'],\n",
    "        'Disco_Avg_Score': train_set['Disco_Avg_Score'],\n",
    "       'Y_true': y_train,\n",
    "       'Y_Pred_Full': y_pred_plus_rfe,}\n",
    "       \n",
    "        \n",
    "output = pd.DataFrame(data)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ebad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('HP1_Plus_EN_Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b77099f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FBgn</th>\n",
       "      <th>log_S2</th>\n",
       "      <th>Density</th>\n",
       "      <th>GC.Promoter</th>\n",
       "      <th>GC.Body</th>\n",
       "      <th>Strand</th>\n",
       "      <th>PInd</th>\n",
       "      <th>state.name_1</th>\n",
       "      <th>state.name_2</th>\n",
       "      <th>state.name_3</th>\n",
       "      <th>...</th>\n",
       "      <th>TTC</th>\n",
       "      <th>TTG</th>\n",
       "      <th>TTT</th>\n",
       "      <th>Mean_Accesibility</th>\n",
       "      <th>DRE_Sites</th>\n",
       "      <th>DRE_Avg_Score</th>\n",
       "      <th>GAGA_Sites</th>\n",
       "      <th>GAGA_Avg_Score</th>\n",
       "      <th>Disco_Sites</th>\n",
       "      <th>Disco_Avg_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FBgn0062565</td>\n",
       "      <td>0.064436</td>\n",
       "      <td>1818</td>\n",
       "      <td>0.536888</td>\n",
       "      <td>0.51</td>\n",
       "      <td>2</td>\n",
       "      <td>2.257467</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.04800</td>\n",
       "      <td>1</td>\n",
       "      <td>8.46970</td>\n",
       "      <td>2</td>\n",
       "      <td>5.606060</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FBgn0053217</td>\n",
       "      <td>0.523959</td>\n",
       "      <td>1002</td>\n",
       "      <td>0.320990</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2</td>\n",
       "      <td>40.465151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3.53125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FBgn0040372</td>\n",
       "      <td>0.243543</td>\n",
       "      <td>1542</td>\n",
       "      <td>0.419050</td>\n",
       "      <td>0.47</td>\n",
       "      <td>2</td>\n",
       "      <td>7.004761</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2.59850</td>\n",
       "      <td>2</td>\n",
       "      <td>9.12121</td>\n",
       "      <td>3</td>\n",
       "      <td>3.783837</td>\n",
       "      <td>1</td>\n",
       "      <td>13.48390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBgn0000316</td>\n",
       "      <td>0.789471</td>\n",
       "      <td>1542</td>\n",
       "      <td>0.421467</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2</td>\n",
       "      <td>5.660925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.76900</td>\n",
       "      <td>2</td>\n",
       "      <td>15.22730</td>\n",
       "      <td>3</td>\n",
       "      <td>5.022220</td>\n",
       "      <td>1</td>\n",
       "      <td>12.32260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FBgn0024989</td>\n",
       "      <td>-1.916831</td>\n",
       "      <td>1542</td>\n",
       "      <td>0.366266</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2</td>\n",
       "      <td>69.763614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2.12500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>5.345455</td>\n",
       "      <td>2</td>\n",
       "      <td>10.39516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          FBgn    log_S2  Density  GC.Promoter  GC.Body  Strand       PInd  \\\n",
       "0  FBgn0062565  0.064436     1818     0.536888     0.51       2   2.257467   \n",
       "1  FBgn0053217  0.523959     1002     0.320990     0.35       2  40.465151   \n",
       "2  FBgn0040372  0.243543     1542     0.419050     0.47       2   7.004761   \n",
       "3  FBgn0000316  0.789471     1542     0.421467     0.34       2   5.660925   \n",
       "4  FBgn0024989 -1.916831     1542     0.366266     0.36       2  69.763614   \n",
       "\n",
       "   state.name_1  state.name_2  state.name_3  ...  TTC  TTG  TTT  \\\n",
       "0             0             0             0  ...    2    4    3   \n",
       "1             0             0             0  ...    2    1    6   \n",
       "2             0             0             0  ...    4    6   10   \n",
       "3             0             0             0  ...    4    2    4   \n",
       "4             0             0             0  ...    1    5    3   \n",
       "\n",
       "   Mean_Accesibility  DRE_Sites  DRE_Avg_Score  GAGA_Sites  GAGA_Avg_Score  \\\n",
       "0            2.04800          1        8.46970           2        5.606060   \n",
       "1            3.53125          0        0.00000           0        0.000000   \n",
       "2            2.59850          2        9.12121           3        3.783837   \n",
       "3            0.76900          2       15.22730           3        5.022220   \n",
       "4            2.12500          0        0.00000           2        5.345455   \n",
       "\n",
       "   Disco_Sites  Disco_Avg_Score  \n",
       "0            0          0.00000  \n",
       "1            0          0.00000  \n",
       "2            1         13.48390  \n",
       "3            1         12.32260  \n",
       "4            2         10.39516  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now do a genomics model without HP1\n",
    "nohp1_data = pd.read_csv('no-hp1-model-data.csv')\n",
    "nohp1_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e6024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/pandas/core/frame.py:4117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Density</th>\n",
       "      <th>GC.Promoter</th>\n",
       "      <th>GC.Body</th>\n",
       "      <th>Strand</th>\n",
       "      <th>PInd</th>\n",
       "      <th>state.name_1</th>\n",
       "      <th>state.name_2</th>\n",
       "      <th>state.name_3</th>\n",
       "      <th>state.name_4</th>\n",
       "      <th>state.name_5</th>\n",
       "      <th>...</th>\n",
       "      <th>TTC</th>\n",
       "      <th>TTG</th>\n",
       "      <th>TTT</th>\n",
       "      <th>Mean_Accesibility</th>\n",
       "      <th>DRE_Sites</th>\n",
       "      <th>DRE_Avg_Score</th>\n",
       "      <th>GAGA_Sites</th>\n",
       "      <th>GAGA_Avg_Score</th>\n",
       "      <th>Disco_Sites</th>\n",
       "      <th>Disco_Avg_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7828</th>\n",
       "      <td>912</td>\n",
       "      <td>0.396569</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1</td>\n",
       "      <td>10.452183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>15.0175</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>1521</td>\n",
       "      <td>0.333945</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2</td>\n",
       "      <td>2.493440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>16.4100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8981</th>\n",
       "      <td>896</td>\n",
       "      <td>0.359779</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>2.380867</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3.5525</td>\n",
       "      <td>2</td>\n",
       "      <td>3.34091</td>\n",
       "      <td>1</td>\n",
       "      <td>5.74545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>2</td>\n",
       "      <td>0.410679</td>\n",
       "      <td>0.27</td>\n",
       "      <td>2</td>\n",
       "      <td>0.052570</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>22.3575</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8810</th>\n",
       "      <td>1812</td>\n",
       "      <td>0.500994</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1.061447</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5755</td>\n",
       "      <td>2</td>\n",
       "      <td>11.26515</td>\n",
       "      <td>1</td>\n",
       "      <td>1.49697</td>\n",
       "      <td>3</td>\n",
       "      <td>6.23118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Density  GC.Promoter  GC.Body  Strand       PInd  state.name_1  \\\n",
       "7828      912     0.396569     0.41       1  10.452183             0   \n",
       "2456     1521     0.333945     0.31       2   2.493440             0   \n",
       "8981      896     0.359779     0.43       1   2.380867             0   \n",
       "2455        2     0.410679     0.27       2   0.052570             0   \n",
       "8810     1812     0.500994     0.44       1   1.061447             0   \n",
       "\n",
       "      state.name_2  state.name_3  state.name_4  state.name_5  ...  TTC  TTG  \\\n",
       "7828             0             1             0             0  ...    2    2   \n",
       "2456             0             0             0             0  ...    2    4   \n",
       "8981             1             0             0             0  ...    4    2   \n",
       "2455             1             0             0             0  ...    1    8   \n",
       "8810             0             0             0             1  ...    5    4   \n",
       "\n",
       "      TTT  Mean_Accesibility  DRE_Sites  DRE_Avg_Score  GAGA_Sites  \\\n",
       "7828   10            15.0175          0        0.00000           0   \n",
       "2456   12            16.4100          0        0.00000           0   \n",
       "8981    7             3.5525          2        3.34091           1   \n",
       "2455   12            22.3575          0        0.00000           0   \n",
       "8810    1             3.5755          2       11.26515           1   \n",
       "\n",
       "      GAGA_Avg_Score  Disco_Sites  Disco_Avg_Score  \n",
       "7828         0.00000            0          0.00000  \n",
       "2456         0.00000            0          0.00000  \n",
       "8981         5.74545            0          0.00000  \n",
       "2455         0.00000            0          0.00000  \n",
       "8810         1.49697            3          6.23118  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nohp1_copy = nohp1_data.copy()\n",
    "# Remove these variables until they are scaled properly\n",
    "#nohp1_copy = nohp1_copy.drop(['Length', 'PInd'], axis = 1)\n",
    "\n",
    "nohp1_train, nohp1_test = train_test_split(nohp1_copy, test_size = 0.2,\n",
    "                                       random_state = 42)\n",
    "\n",
    "\n",
    "no_y_train = nohp1_train['log_S2']\n",
    "no_y_test = nohp1_test['log_S2']\n",
    "\n",
    "for set_ in (nohp1_train, nohp1_test):\n",
    "    set_.drop(\"log_S2\", axis=1, inplace=True)\n",
    "\n",
    "    # Just keeping a version of each of these that retains the FBgn\n",
    "nohp1_train2 = nohp1_train.drop(['FBgn'], axis = 1)\n",
    "nohp1_test2 = nohp1_test.drop(['FBgn'], axis = 1)\n",
    "\n",
    "for (columnName, columnData) in nohp1_train2.iteritems():\n",
    "    model_data[columnName] = model_data[columnName].astype('float32')\n",
    "    \n",
    "nohp1_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9adc92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'l1_ratio': 0.1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "elastic_net_gsearch.fit(nohp1_train2, no_y_train)\n",
    "print(elastic_net_gsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a6091de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['state.name_1', 'state.name_2', 'state.name_3', 'state.name_4',\n",
       "       'state.name_9', 'CTC', 'GGA', 'GGC', 'GTC', 'TCC', 'Mean_Accesibility',\n",
       "       'DRE_Sites', 'DRE_Avg_Score', 'Disco_Sites', 'Disco_Avg_Score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe_selector2 = RFE(estimator=ElasticNet(alpha = 0.1, l1_ratio = .1),\n",
    "                   n_features_to_select = 15, step = 1)\n",
    "rfe_selector2.fit(nohp1_train2, no_y_train)\n",
    "\n",
    "nohp1_train2.columns[rfe_selector2.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1bc840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120.639228182539\n",
      "5099.058965551181\n"
     ]
    }
   ],
   "source": [
    "rfe_no_train = nohp1_train2[nohp1_train2.columns[rfe_selector2.get_support()]]\n",
    "\n",
    "no_y_pred_rfe = cross_val_predict(enet, rfe_no_train, no_y_train, cv=10)\n",
    "\n",
    "resid_rfe = no_y_pred_rfe - y_train\n",
    "\n",
    "no_y_pred = cross_val_predict(enet, nohp1_train2, no_y_train, cv=10)\n",
    "no_resid = no_y_pred - no_y_train\n",
    "\n",
    "print(sum(resid_rfe**2))\n",
    "print(sum(no_resid**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24a62275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FBgn</th>\n",
       "      <th>Y_true</th>\n",
       "      <th>Y_Pred_Full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7828</th>\n",
       "      <td>FBgn0260960</td>\n",
       "      <td>1.422502</td>\n",
       "      <td>0.475136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>FBgn0052000</td>\n",
       "      <td>0.097486</td>\n",
       "      <td>0.540815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8981</th>\n",
       "      <td>FBgn0004228</td>\n",
       "      <td>-1.276521</td>\n",
       "      <td>0.157085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>FBgn0005278</td>\n",
       "      <td>0.798414</td>\n",
       "      <td>0.740574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8810</th>\n",
       "      <td>FBgn0031053</td>\n",
       "      <td>0.670263</td>\n",
       "      <td>0.825742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             FBgn    Y_true  Y_Pred_Full\n",
       "7828  FBgn0260960  1.422502     0.475136\n",
       "2456  FBgn0052000  0.097486     0.540815\n",
       "8981  FBgn0004228 -1.276521     0.157085\n",
       "2455  FBgn0005278  0.798414     0.740574\n",
       "8810  FBgn0031053  0.670263     0.825742"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write out predictions for this dataset\n",
    "data = {'FBgn': nohp1_train['FBgn'],\n",
    "       'Y_true': no_y_train,\n",
    "       'Y_Pred_Full': no_y_pred_rfe,}\n",
    "       \n",
    "        \n",
    "output = pd.DataFrame(data)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5917e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('Genome_Baseline_Model_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "113a838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jack/miniconda3/envs/tf2/lib/python3.7/site-packages/pandas/core/frame.py:4117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HP1a</th>\n",
       "      <th>HP1B</th>\n",
       "      <th>HP1C</th>\n",
       "      <th>A_B</th>\n",
       "      <th>A_C</th>\n",
       "      <th>B_C</th>\n",
       "      <th>A_B_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7828</th>\n",
       "      <td>7.174321</td>\n",
       "      <td>1.976924</td>\n",
       "      <td>34.705778</td>\n",
       "      <td>9.563371</td>\n",
       "      <td>17.555440</td>\n",
       "      <td>4.837500</td>\n",
       "      <td>3.629032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>3.931411</td>\n",
       "      <td>4.036933</td>\n",
       "      <td>0.973861</td>\n",
       "      <td>1.602246</td>\n",
       "      <td>1.645251</td>\n",
       "      <td>2.453688</td>\n",
       "      <td>2.389551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8981</th>\n",
       "      <td>-0.056331</td>\n",
       "      <td>0.314880</td>\n",
       "      <td>-0.256333</td>\n",
       "      <td>0.219757</td>\n",
       "      <td>0.045857</td>\n",
       "      <td>-1.228404</td>\n",
       "      <td>-0.178896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>2.214164</td>\n",
       "      <td>6.897567</td>\n",
       "      <td>4.281574</td>\n",
       "      <td>3.115202</td>\n",
       "      <td>1.933721</td>\n",
       "      <td>6.023930</td>\n",
       "      <td>13.337967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8810</th>\n",
       "      <td>0.784784</td>\n",
       "      <td>1.988963</td>\n",
       "      <td>4.032536</td>\n",
       "      <td>2.027456</td>\n",
       "      <td>1.560908</td>\n",
       "      <td>3.164672</td>\n",
       "      <td>1.591116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          HP1a      HP1B       HP1C       A_B        A_C       B_C      A_B_C\n",
       "7828  7.174321  1.976924  34.705778  9.563371  17.555440  4.837500   3.629032\n",
       "2456  3.931411  4.036933   0.973861  1.602246   1.645251  2.453688   2.389551\n",
       "8981 -0.056331  0.314880  -0.256333  0.219757   0.045857 -1.228404  -0.178896\n",
       "2455  2.214164  6.897567   4.281574  3.115202   1.933721  6.023930  13.337967\n",
       "8810  0.784784  1.988963   4.032536  2.027456   1.560908  3.164672   1.591116"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are models robust to randomly permuted data?\n",
    "mlplus_rand = pd.read_csv('HP1plus_randomized.csv')\n",
    "hp1only_rand = pd.read_csv('HP1only_randomized.csv')\n",
    "nohp1_rand = pd.read_csv('NoHP1_randomized.csv')\n",
    "\n",
    "rplus_train, rplus_test = train_test_split(mlplus_rand, test_size = 0.2,\n",
    "                                       random_state = 42)\n",
    "rhp1_train, rhp1_test = train_test_split(hp1only_rand, test_size = 0.2,\n",
    "                                       random_state = 42)\n",
    "rno_train, rno_test = train_test_split(nohp1_rand, test_size = 0.2,\n",
    "                                       random_state = 42)\n",
    "\n",
    "rplus_y = rplus_train['log_S2']\n",
    "rhp1_y = rhp1_train['log_S2']\n",
    "rno_y = rno_train['log_S2']\n",
    "\n",
    "for set_ in (rplus_train, rhp1_train, rno_train):\n",
    "    set_.drop(\"log_S2\", axis=1, inplace=True)\n",
    "    \n",
    "rhp1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74a62221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.9, 'l1_ratio': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# rplus grid search\n",
    "elastic_net_gsearch.fit(rplus_train, rplus_y)\n",
    "print(elastic_net_gsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f5878f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HP1B', 'T', 'AC', 'CA', 'CT', 'TG', 'AAG', 'AAT', 'ACA', 'ACC', 'ATA',\n",
       "       'GAT', 'GTA', 'TAA', 'TCT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe_selector = RFE(estimator=ElasticNet(alpha = 0.9, l1_ratio = .9),\n",
    "                   n_features_to_select = 15, step = 1)\n",
    "rfe_selector.fit(rplus_train, rplus_y)\n",
    "\n",
    "rplus_train.columns[rfe_selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "596c1027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9054.864271710625\n"
     ]
    }
   ],
   "source": [
    "# Randomized mlplus model performance\n",
    "rfe_rplus = rplus_train[rplus_train.columns[rfe_selector.get_support()]]\n",
    "rplus_net = ElasticNet(alpha = 0.9, l1_ratio = 0.9)\n",
    "\n",
    "rplus_y_pred = cross_val_predict(rplus_net, rfe_rplus, rplus_y, cv=10)\n",
    "\n",
    "rplus_resid = rplus_y_pred - rplus_y\n",
    "\n",
    "print(sum(rplus_resid**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87714b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.9, 'l1_ratio': 0.9}\n"
     ]
    }
   ],
   "source": [
    "elastic_net_gsearch.fit(rno_train, rno_y)\n",
    "print(elastic_net_gsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06e5bca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Density', 'Strand', 'state.name_2', 'state.name_5', 'C', 'AG', 'CG',\n",
       "       'AAG', 'ATT', 'CAA', 'CCA', 'TAC', 'TCG', 'TTT', 'Disco_Avg_Score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe_selector = RFE(estimator=ElasticNet(alpha = 0.9, l1_ratio = .9),\n",
    "                   n_features_to_select = 15, step = 1)\n",
    "rfe_selector.fit(rno_train, rno_y)\n",
    "\n",
    "rno_train.columns[rfe_selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36d99e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9089.705418069998\n"
     ]
    }
   ],
   "source": [
    "# Randomized no hp1 model performance\n",
    "rfe_rno = rno_train[rno_train.columns[rfe_selector.get_support()]]\n",
    "\n",
    "# Parameters are the same\n",
    "rno_y_pred = cross_val_predict(rplus_net, rfe_rno, rno_y, cv=10)\n",
    "\n",
    "rno_resid = rno_y_pred - rno_y\n",
    "\n",
    "print(sum(rno_resid**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6f15257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9098.801037789628\n"
     ]
    }
   ],
   "source": [
    "# Randomized HP1 only model performance\n",
    "rhp1_y_pred = cross_val_predict(rplus_net, rhp1_train, rhp1_y, cv=10)\n",
    "\n",
    "rhp1_resid = rhp1_y_pred - rhp1_y\n",
    "\n",
    "print(sum(rhp1_resid**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bfd651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now - write outputs for randomized datasets\n",
    "# HP1 plus permutation output\n",
    "rplus_out = {'Y_pred': rhp1_y_pred,\n",
    "             'Y_true': rplus_y}\n",
    "rplus_out_df = pd.DataFrame(rplus_out)\n",
    "rplus_out_df.to_csv('HP1_Plus_Permutation_output.csv', index = False)\n",
    "\n",
    "# No HP1 permutation output\n",
    "rno_out = {'Y_pred': rno_y_pred,\n",
    "          'Y_true': rno_y}\n",
    "rno_out_df = pd.DataFrame(rno_out)\n",
    "rno_out_df.to_csv('No_HP1_Permutation_output.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f911122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sets\n",
    "# HP1 Plus\n",
    "# Fit model on entire training set\n",
    "hp1_plus_enet = ElasticNet(alpha = 0.1, l1_ratio = 0.1)\n",
    "\n",
    "\n",
    "hp1_plus_enet.fit(train_set2[train_set2.columns[rfe_selector.get_support()]], y_train)\n",
    "rfe_test_plus = test_set2[train_set2.columns[rfe_selector.get_support()]]\n",
    "\n",
    "hp1_plus_y_test_preds = hp1_plus_enet.predict(rfe_test_plus)\n",
    "hp1_plus_test_out = {'Y_true': y_test,\n",
    "                    'Y_pred': hp1_plus_y_test_preds,\n",
    "                    'FBgn': test_set['FBgn']}\n",
    "pd.DataFrame(hp1_plus_test_out).to_csv('HP1_Plus_Model_Test_Predictions.csv', index = False)\n",
    "\n",
    "no_hp1_enet = ElasticNet(alpha = 0.1, l1_ratio = 0.1)\n",
    "no_hp1_enet.fit(nohp1_train2[nohp1_train2.columns[rfe_selector2.get_support()]], no_y_train)\n",
    "rfe_test_no = nohp1_test2[nohp1_train2.columns[rfe_selector2.get_support()]]\n",
    "\n",
    "no_hp1_test_preds = no_hp1_enet.predict(rfe_test_no)\n",
    "no_test_out = {'Y_true': no_y_test,\n",
    "               'Y_pred': no_hp1_test_preds,\n",
    "               'FBgn': nohp1_test['FBgn']}\n",
    "pd.DataFrame(no_test_out).to_csv('No_HP1_Model_Test_Predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad7aae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "hp1_plus_coefs = pd.DataFrame({'Features': train_set2.columns[rfe_selector.get_support()],\n",
    "                              'Coefficient': hp1_plus_enet.coef_})\n",
    "hp1_plus_coefs.to_csv('HP1_Plus_Model_Coefficients.csv', index = False)\n",
    "\n",
    "no_hp1_coefs = pd.DataFrame({'Features': nohp1_train2.columns[rfe_selector2.get_support()],\n",
    "                           'Coefficient': no_hp1_enet.coef_})\n",
    "no_hp1_coefs.to_csv('No_HP1_Model_Coefficients.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e62ea2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulations\n",
    "\n",
    "dgt3_a = pd.read_csv('dgt3_HP1a_HP1Plus_sim.csv')\n",
    "dgt3_b = pd.read_csv('dgt3_HP1B_HP1Plus_sim.csv')\n",
    "dgt3_c = pd.read_csv('dgt3_HP1C_HP1Plus_sim.csv')\n",
    "\n",
    "a_dgt3_preds = hp1_plus_enet.predict(dgt3_a)\n",
    "b_dgt3_preds = hp1_plus_enet.predict(dgt3_b)\n",
    "c_dgt3_preds = hp1_plus_enet.predict(dgt3_c)\n",
    "\n",
    "dgt3_predictions = pd.DataFrame({'HP1a': a_dgt3_preds,\n",
    "                                'HP1B': b_dgt3_preds,\n",
    "                                'HP1C': c_dgt3_preds})\n",
    "dgt3_predictions.to_csv('dgt3_HP1Plus_sim_predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4bb927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpr11b_a = pd.read_csv('Cpr11B_HP1a_HP1Plus_sim.csv')\n",
    "cpr11b_b = pd.read_csv('Cpr11B_HP1B_HP1Plus_sim.csv')\n",
    "cpr11b_c = pd.read_csv('Cpr11B_HP1C_HP1Plus_sim.csv')\n",
    "\n",
    "a_Cpr11B_preds = hp1_plus_enet.predict(cpr11b_a)\n",
    "b_Cpr11B_preds = hp1_plus_enet.predict(cpr11b_b)\n",
    "c_Cpr11B_preds = hp1_plus_enet.predict(cpr11b_c)\n",
    "\n",
    "Cpr11B_predictions = pd.DataFrame({'HP1a': a_Cpr11B_preds,\n",
    "                                'HP1B': b_Cpr11B_preds,\n",
    "                                'HP1C': c_Cpr11B_preds})\n",
    "Cpr11B_predictions.to_csv('Cpr11B_HP1Plus_sim_predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7de6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceca1_a = pd.read_csv('CecA1_HP1a_HP1Plus_sim.csv')\n",
    "ceca1_b = pd.read_csv('CecA1_HP1B_HP1Plus_sim.csv')\n",
    "ceca1_c = pd.read_csv('CecA1_HP1C_HP1Plus_sim.csv')\n",
    "\n",
    "a_CecA1_preds = hp1_plus_enet.predict(ceca1_a)\n",
    "b_CecA1_preds = hp1_plus_enet.predict(ceca1_b)\n",
    "c_CecA1_preds = hp1_plus_enet.predict(ceca1_c)\n",
    "\n",
    "CecA1_predictions = pd.DataFrame({'HP1a': a_CecA1_preds,\n",
    "                                'HP1B': b_CecA1_preds,\n",
    "                                'HP1C': c_CecA1_preds})\n",
    "CecA1_predictions.to_csv('CecA1_HP1Plus_sim_predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9c34bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtk_a = pd.read_csv('Mtk_HP1a_HP1Plus_sim.csv')\n",
    "mtk_b = pd.read_csv('Mtk_HP1B_HP1Plus_sim.csv')\n",
    "mtk_c = pd.read_csv('Mtk_HP1C_HP1Plus_sim.csv')\n",
    "\n",
    "a_Mtk_preds = hp1_plus_enet.predict(cpr11b_a)\n",
    "b_Mtk_preds = hp1_plus_enet.predict(cpr11b_b)\n",
    "c_Mtk_preds = hp1_plus_enet.predict(cpr11b_c)\n",
    "\n",
    "Mtk_predictions = pd.DataFrame({'HP1a': a_Mtk_preds,\n",
    "                                'HP1B': b_Mtk_preds,\n",
    "                                'HP1C': c_Mtk_preds})\n",
    "Mtk_predictions.to_csv('Mtk_HP1Plus_sim_predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d5622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
